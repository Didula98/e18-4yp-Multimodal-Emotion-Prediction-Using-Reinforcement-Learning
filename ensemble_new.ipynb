{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiyumaliSandunika/e18-4yp-Multimodal-Emotion-Prediction-Using-Reinforcement-Learning/blob/main/ensemble_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "4nS-UJzVy2Si"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import librosa\n",
        "import soundfile\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "emotions = {'01': 'Happy', '02': 'Sad', '03': 'Angry', '04': 'Neutral'}\n",
        "observed_emotions = observed_emotions = ['happiness', 'sadness', 'anger', 'neutral']\n",
        "\n",
        "def extract_feature(file_name, mfcc, chroma, mel):\n",
        "    X, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
        "    if chroma:\n",
        "        stft = np.abs(librosa.stft(X))\n",
        "    result = np.array([])\n",
        "    if mfcc:\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "        result = np.hstack((result, mfccs))\n",
        "    if chroma:\n",
        "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "        result = np.hstack((result, chroma))\n",
        "    if mel:\n",
        "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
        "        result = np.hstack((result, mel))\n",
        "    return result\n",
        "\n",
        "def predict_one(model,audio_file):\n",
        "    # model = joblib.load(\"TrainedUsing95percCommonData&IncLearnedUsingPersAllDataBatchSize1.pkl\")\n",
        "    feature = extract_feature(audio_file, mfcc=True, chroma=True, mel=True)\n",
        "    x = np.array([feature])\n",
        "    pred = model.predict(x)\n",
        "    y_pred_logits = model.predict_proba(np.array(x))\n",
        "    probabilities_list = y_pred_logits.squeeze().tolist()\n",
        "    print(probabilities_list)\n",
        "    print(pred)\n",
        "    return probabilities_list, pred\n",
        "\n",
        "def predict_batch(model,folder_path):\n",
        "    # model = joblib.load(\"TrainedUsing95percCommonData&IncLearnedUsingPersAllDataBatchSize1.pkl\")\n",
        "    x, y_true = [], []\n",
        "    for folder in observed_emotions:\n",
        "        filepath = os.path.join(folder_path, folder)\n",
        "        for filename in os.listdir(filepath):\n",
        "            feature = extract_feature(os.path.join(filepath, filename), mfcc=True, chroma=True, mel=True)\n",
        "            x.append(feature)\n",
        "            y_true.append(folder)\n",
        "    y_pred_logits = model.predict_proba(np.array(x))\n",
        "    return y_pred_logits, y_true\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HhmF2TkADZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install resampy"
      ],
      "metadata": {
        "id": "dhCInWblyFou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa6UQz_xzCeG",
        "outputId": "70bbd2d2-1c14-43c3-ae11-fe3b098bd05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "xeMzADORzD0I"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "import torch\n",
        "import pickle\n",
        "import joblib\n",
        "# Assuming device is either \"cpu\" or \"cuda:0\"\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_path = \"/content/drive/MyDrive/FYP_Text/ERM_30.pkl\"\n",
        "text_model = joblib.load(model_path)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "VXCeDzAvzLE3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def predict_emotion_with_probabilities(sentence, model, tokenizer, device):\n",
        "    # Tokenize the input sentence\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits.to(device)\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    # Convert probabilities tensor to list\n",
        "    probabilities_list = probabilities.squeeze().tolist()\n",
        "\n",
        "    # Map predicted label to emotion\n",
        "    emotion_labels = [\"happiness\", \"sadness\", \"anger\", \"neutral\"]\n",
        "    predicted_emotion = emotion_labels[torch.argmax(logits, dim=1).item()]\n",
        "\n",
        "    return probabilities_list, predicted_emotion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn8ngnjU7cet",
        "outputId": "c78b72a8-0216-4fba-9ea4-6c235481f9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.047014032734361e-10, 0.9999999950462839, 5.143119161296012e-11, 4.397583581577403e-09]\n",
            "['Happy']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelBinarizer from version 1.3.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MLPClassifier from version 1.3.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "sentence = \"Yesterday was good\"\n",
        "audio_file = '/content/drive/MyDrive/FYP_Text/audio_dataset/Happy/Happy_out_8.wav'\n",
        "model_vocal = joblib.load(\"/content/drive/MyDrive/FYP_Text/TrainedUsing95percCommonData&IncLearnedUsingPersAllDataBatchSize1.pkl\")\n",
        "prob1, _ = predict_emotion_with_probabilities(sentence, text_model, tokenizer, device)\n",
        "prob2, _ = predict_one(model_vocal,audio_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh5HgOPJ8xx-",
        "outputId": "e42a505a-55d5-49e5-f77a-f8c078b6e64a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9999516010284424, 3.439402280491777e-05, 3.1837571441428736e-06, 1.077303932106588e-05]\n",
            "[5.047014032734361e-10, 0.9999999950462839, 5.143119161296012e-11, 4.397583581577403e-09]\n"
          ]
        }
      ],
      "source": [
        "print(prob1)\n",
        "print(prob2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OP7zAAq9p8p"
      },
      "outputs": [],
      "source": [
        "# def ensemble_predictions(probabilities1,probabilities2, model_weights=None):\n",
        "\n",
        "#     # Apply model-wise weights if provided\n",
        "#     if model_weights:\n",
        "#         probabilities1_ = [p1 * w for p1, w in zip(probabilities1, model_weights[0])]\n",
        "#         probabilities2_ = [p2 * w for p2, w in zip(probabilities2, model_weights[1])]\n",
        "\n",
        "#     # # Apply class-wise weights if provided\n",
        "#     # if class_weights:\n",
        "#     #     probabilities1 = [p1 * w for p1, w in zip(probabilities1, class_weights)]\n",
        "#     #     probabilities2 = [p2 * w for p2, w in zip(probabilities2, class_weights)]\n",
        "\n",
        "#     # Calculate ensemble probabilities by averaging\n",
        "#     ensemble_probabilities = [(p1 + p2) / 2 for p1, p2 in zip(probabilities1_, probabilities2_)]\n",
        "\n",
        "#     # Map ensemble probabilities to emotion labels\n",
        "#     emotion_labels = [\"happiness\", \"sadness\", \"anger\", \"neutral\"]\n",
        "#     predicted_emotion = emotion_labels[np.argmax(ensemble_probabilities)]\n",
        "\n",
        "#     return probabilities1, probabilities2, ensemble_probabilities, predicted_emotion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "NI2huMn9FZ94"
      },
      "outputs": [],
      "source": [
        "def ensemble_predictions(probabilities1, probabilities2, user_labels, model_weights, learning_rate=0.1):\n",
        "    # Apply model-wise weights if provided\n",
        "    # if model_weights:\n",
        "    # probabilities1_ = [p1 * w for p1, w in zip(probabilities1, model_weights[0])]\n",
        "    # probabilities2_ = [p2 * w for p2, w in zip(probabilities2, model_weights[1])]\n",
        "\n",
        "    probabilities1_ = [p1 * model_weights[0] for p1 in probabilities1]\n",
        "    probabilities2_ = [p2 * model_weights[1] for p2 in probabilities2]\n",
        "    # Calculate ensemble probabilities by averaging\n",
        "    ensemble_probabilities = [(p1 + p2) / 2 for p1, p2 in zip(probabilities1_, probabilities2_)]\n",
        "\n",
        "    # Map ensemble probabilities to emotion labels\n",
        "    emotion_labels = [\"happiness\", \"sadness\", \"anger\", \"neutral\"]\n",
        "    predicted_emotion1 = emotion_labels[np.argmax(probabilities1_)]\n",
        "    print(predicted_emotion1)\n",
        "    predicted_emotion2 = emotion_labels[np.argmax(probabilities2_)]\n",
        "    print(predicted_emotion2)\n",
        "    predicted_emotion  = emotion_labels[np.argmax(ensemble_probabilities)]\n",
        "    # Update model weights using online learning\n",
        "    # if model_weights:\n",
        "    #     # Update weights based on whether the predicted emotion matches the user-provided label\n",
        "    #     for i, (p1, p2, user_label) in enumerate(zip(probabilities1, probabilities2, user_labels)):\n",
        "    #         print(user_label)\n",
        "    #         print(predicted_emotion1)\n",
        "    if predicted_emotion1 == user_labels:\n",
        "\n",
        "        # Increase weights for correct predictions\n",
        "        model_weights[0] += learning_rate\n",
        "    else:\n",
        "        model_weights[0] -= learning_rate\n",
        "\n",
        "    if predicted_emotion2 == user_labels:\n",
        "        model_weights[1] += learning_rate\n",
        "    else:\n",
        "        model_weights[1] -= learning_rate\n",
        "\n",
        "\n",
        "    return probabilities1, probabilities2, ensemble_probabilities, predicted_emotion, model_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1foQUuA2AL6K",
        "outputId": "3513339a-b97c-447e-db2b-edb7731362ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happiness\n",
            "sadness\n",
            "Probabilities (Model 1): [0.9999516010284424, 3.439402280491777e-05, 3.1837571441428736e-06, 1.077303932106588e-05]\n",
            "Probabilities (Model 2): [5.047014032734361e-10, 0.9999999950462839, 5.143119161296012e-11, 4.397583581577403e-09]\n",
            "Ensemble Probabilities: [0.24998790038328594, 0.2500085972672722, 7.959521438336217e-07, 2.6943592261618643e-06]\n",
            "Ensemble Predicted Emotion: sadness\n",
            "[0.51, 0.49]\n"
          ]
        }
      ],
      "source": [
        "# Example usage with model and class weights\n",
        "# sentence = \"This is a test sentence.\"\n",
        "model_weights = [0.5,0.5]  # Example weights for each model\n",
        "# class_weights = [1.0, 1.0, 1.0,1.0]  # Example weights for each class\n",
        "probabilities1_new, probabilities2_new, ensemble_probabilities, predicted_emotion,new_model_weights = ensemble_predictions(prob1,prob2, 'happiness',model_weights,0.01)\n",
        "print(\"Probabilities (Model 1):\", probabilities1_new)\n",
        "print(\"Probabilities (Model 2):\", probabilities2_new)\n",
        "print(\"Ensemble Probabilities:\", ensemble_probabilities)\n",
        "print(\"Ensemble Predicted Emotion:\", predicted_emotion)\n",
        "print(new_model_weights)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# emotions = {\"happiness\": 0, \"sadness\": 1, \"anger\": 2, \"neutral\": 3}\n",
        "# observed_emotions = ['Happy', 'Sad', 'Angry', 'Neutral']\n",
        "# model_weights = [0.5,0.5]\n",
        "# x, y_label = [], []\n",
        "# folder_path = \"/content/drive/MyDrive/FYP_Text/audio_dataset\"\n",
        "# for folder in observed_emotions:\n",
        "#     filepath = os.path.join(folder_path, folder)\n",
        "#     print(filepath)\n",
        "#     for filename in os.listdir(filepath):\n",
        "#         # print(os.path.join(filepath, filename))\n",
        "#         hj = os.path.join(filepath, filename)\n",
        "#         try:\n",
        "\n",
        "#             transcribed_text = audio_to_text(hj)\n",
        "#             if transcribed_text:\n",
        "#                 print(\"Transcribed Text:\", transcribed_text)\n",
        "#                 # probabilities1_new, probabilities2_new, ensemble_probabilities, predicted_emotion,new_model_weights = ensemble_predictions(prob1,prob2, folder,model_weights,0.01)\n",
        "#                 # print(\"Probabilities (Model 1):\", probabilities1_new)\n",
        "#                 # print(\"Probabilities (Model 2):\", probabilities2_new)\n",
        "#                 # print(\"Ensemble Probabilities:\", ensemble_probabilities)\n",
        "#                 # print(\"Ensemble Predicted Emotion:\", predicted_emotion)\n",
        "#                 # print(new_model_weights)\n",
        "\n",
        "#                 # x.append(transcribed_text)\n",
        "#                 # y_label.append(folder)\n",
        "#         except:\n",
        "#             print(\"Error occurred while processing:\", hj)\n",
        "#         # feature = extract_feature(os.path.join(filepath, filename), mfcc=True, chroma=True, mel=True)\n",
        "#         # x.append(feature)\n",
        "#         # y_true.append(folder)"
      ],
      "metadata": {
        "id": "C7zngHW96ROZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "import torch\n",
        "import pickle\n",
        "import joblib\n",
        "# Assuming device is either \"cpu\" or \"cuda:0\"\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_path = \"/content/drive/MyDrive/FYP_Text/ERM_30.pkl\"\n",
        "text_model = joblib.load(model_path)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "vocal_model = joblib.load(\"/content/drive/MyDrive/FYP_Text/TrainedUsing95percCommonData&IncLearnedUsingPersAllDataBatchSize1.pkl\")\n",
        "\n",
        "emotions = {\"happiness\": 0, \"sadness\": 1, \"anger\": 2, \"neutral\": 3}\n",
        "observed_emotions = ['happiness', 'sadness', 'anger', 'neutral']\n",
        "\n",
        "x, y_label = [], []\n",
        "folder_path = \"/content/drive/MyDrive/FYP_Text/audio_dataset2\"\n",
        "for folder in observed_emotions:\n",
        "    filepath = os.path.join(folder_path, folder)\n",
        "    print(filepath)\n",
        "    for filename in os.listdir(filepath):\n",
        "        # print(os.path.join(filepath, filename))\n",
        "        selected_audio = os.path.join(filepath, filename)\n",
        "        try:\n",
        "\n",
        "            transcribed_text = audio_to_text(selected_audio)\n",
        "\n",
        "            if transcribed_text:\n",
        "                probabilities1, _ = predict_emotion_with_probabilities(transcribed_text, text_model, tokenizer, device)\n",
        "                probabilities2, _ = predict_one(model_vocal,selected_audio)\n",
        "                probabilities1_new, probabilities2_new, ensemble_probabilities, predicted_emotion,weights = ensemble_predictions(probabilities1,probabilities2, folder,model_weights,0.01)\n",
        "                print(\"Probabilities (Model 1):\", probabilities1_new)\n",
        "                print(\"Probabilities (Model 2):\", probabilities2_new)\n",
        "                print(\"Ensemble Probabilities:\", ensemble_probabilities)\n",
        "                print(\"Ensemble Predicted Emotion:\", predicted_emotion)\n",
        "                print(weights)\n",
        "                # print(\"Transcribed Text:\", transcribed_text)\n",
        "                # x.append(transcribed_text)\n",
        "                # y_label.append(folder)\n",
        "\n",
        "        except:\n",
        "            print(\"Error occurred while processing:\", selected_audio)\n",
        "        # feature = extract_feature(os.path.join(filepath, filename), mfcc=True, chroma=True, mel=True)\n",
        "        # x.append(feature)\n",
        "        # y_true.append(folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dko2njn3AhuF",
        "outputId": "0f932108-11e4-48eb-e436-4b67701de0c1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelBinarizer from version 1.3.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MLPClassifier from version 1.3.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FYP_Text/audio_dataset2/happiness\n",
            "[0.14244868895147347, 0.8550219261997825, 8.573480332911055e-06, 0.0025208113684110373]\n",
            "['Happy']\n",
            "happiness\n",
            "sadness\n",
            "Probabilities (Model 1): [0.99992835521698, 4.421417543198913e-06, 2.407367901469115e-06, 6.479084549937397e-05]\n",
            "Probabilities (Model 2): [0.14244868895147347, 0.8550219261997825, 8.573480332911055e-06, 0.0025208113684110373]\n",
            "Ensemble Probabilities: [0.2898816593734409, 0.2094814993804202, 2.714381496437833e-06, 0.0006341204508630446]\n",
            "Ensemble Predicted Emotion: happiness\n",
            "[0.52, 0.48]\n",
            "[0.049786146050709155, 0.9501144197519716, 3.255186674379077e-05, 6.688233057551054e-05]\n",
            "['Happy']\n",
            "happiness\n",
            "sadness\n",
            "Probabilities (Model 1): [0.9999876022338867, 3.502062099869363e-06, 1.5985957588782185e-06, 7.352874490607064e-06]\n",
            "Probabilities (Model 2): [0.049786146050709155, 0.9501144197519716, 3.255186674379077e-05, 6.688233057551054e-05]\n",
            "Ensemble Probabilities: [0.27194545163298073, 0.22802837127661915, 8.22808291581812e-06, 1.796350670568037e-05]\n",
            "Ensemble Predicted Emotion: happiness\n",
            "[0.53, 0.47]\n",
            "[0.09765409259759425, 0.9013046119112814, 0.0010238017824800271, 1.7493708644262052e-05]\n",
            "['Happy']\n",
            "happiness\n",
            "sadness\n",
            "Probabilities (Model 1): [0.9999879598617554, 4.293924575904384e-06, 1.0858937002922175e-06, 6.702875452901935e-06]\n",
            "Probabilities (Model 2): [0.09765409259759425, 0.9013046119112814, 0.0010238017824800271, 1.7493708644262052e-05]\n",
            "Ensemble Probabilities: [0.28794552112379984, 0.21180772168916373, 0.0002408811807133838, 5.887283526420595e-06]\n",
            "Ensemble Predicted Emotion: happiness\n",
            "[0.54, 0.45999999999999996]\n",
            "/content/drive/MyDrive/FYP_Text/audio_dataset2/sadness\n",
            "[1.3438268743784279e-09, 5.032585615536214e-07, 3.931705328219452e-09, 0.9999994914659062]\n",
            "['Sad']\n",
            "sadness\n",
            "neutral\n",
            "Probabilities (Model 1): [1.520567707302689e-06, 0.9999946355819702, 1.4325132724479772e-06, 2.4217702048190404e-06]\n",
            "Probabilities (Model 2): [1.3438268743784279e-09, 5.032585615536214e-07, 3.931705328219452e-09, 0.9999994914659062]\n",
            "Ensemble Probabilities: [4.108623611528331e-07, 0.26999866735660116, 3.8768287578644435e-07, 0.2300005369151137]\n",
            "Ensemble Predicted Emotion: sadness\n",
            "[0.55, 0.44999999999999996]\n",
            "[2.7164073869242235e-08, 7.033869420494544e-06, 2.047431390475827e-07, 0.9999927342233667]\n",
            "['Sad']\n",
            "neutral\n",
            "neutral\n",
            "Probabilities (Model 1): [3.948595349356765e-06, 3.327483909743023e-06, 1.800472773538786e-06, 0.9999909400939941]\n",
            "Probabilities (Model 2): [2.7164073869242235e-08, 7.033869420494544e-06, 2.047431390475827e-07, 0.9999927342233667]\n",
            "Ensemble Probabilities: [1.09197563769369e-06, 2.4976786947906034e-06, 5.411972190088723e-07, 0.4999958737261059]\n",
            "Ensemble Predicted Emotion: neutral\n",
            "[0.54, 0.43999999999999995]\n",
            "[2.883987864635883e-05, 0.0008347558879947059, 1.156605165514626e-06, 0.9991352476281935]\n",
            "['Sad']\n",
            "anger\n",
            "neutral\n",
            "Probabilities (Model 1): [1.4233366982807638e-06, 1.4540646589011885e-05, 0.9999796152114868, 4.36093432654161e-06]\n",
            "Probabilities (Model 2): [2.883987864635883e-05, 0.0008347558879947059, 1.156605165514626e-06, 0.9991352476281935]\n",
            "Ensemble Probabilities: [6.729074210734748e-06, 0.00018757226993786847, 0.26999475056023786, 0.21981093193047072]\n",
            "Ensemble Predicted Emotion: anger\n",
            "[0.53, 0.42999999999999994]\n",
            "/content/drive/MyDrive/FYP_Text/audio_dataset2/anger\n",
            "[0.9813998269424951, 0.01859985503564051, 2.614871881993119e-07, 5.6534676120282396e-08]\n",
            "['Angry']\n",
            "neutral\n",
            "happiness\n",
            "Probabilities (Model 1): [4.2281608330085874e-05, 0.00037073399289511144, 5.528154815692687e-06, 0.999581515789032]\n",
            "Probabilities (Model 2): [0.9813998269424951, 0.01859985503564051, 2.614871881993119e-07, 5.6534676120282396e-08]\n",
            "Ensemble Probabilities: [0.21101216741884388, 0.0040972133407799136, 1.5211807716214142e-06, 0.26488911383904884]\n",
            "Ensemble Predicted Emotion: neutral\n",
            "[0.52, 0.41999999999999993]\n",
            "[0.9999999589189285, 4.0947542037565074e-08, 1.1900232463156058e-15, 1.3352835912630398e-10]\n",
            "['Angry']\n",
            "anger\n",
            "happiness\n",
            "Probabilities (Model 1): [1.6273564824587083e-06, 1.9493159925332293e-05, 0.9999680519104004, 1.0753077731351368e-05]\n",
            "Probabilities (Model 2): [0.9999999589189285, 4.0947542037565074e-08, 1.1900232463156058e-15, 1.3352835912630398e-10]\n",
            "Ensemble Probabilities: [0.2100004144856604, 5.076820564414285e-06, 0.25999169349670437, 2.7958282511067724e-06]\n",
            "Ensemble Predicted Emotion: anger\n",
            "[0.53, 0.4099999999999999]\n",
            "[0.9999870290402225, 1.0369154043284095e-05, 2.4219269229612533e-10, 2.601563541560286e-06]\n",
            "['Angry']\n",
            "sadness\n",
            "happiness\n",
            "Probabilities (Model 1): [1.78198406501906e-05, 0.9712001085281372, 0.028753872960805893, 2.817439781210851e-05]\n",
            "Probabilities (Model 2): [0.9999870290402225, 1.0369154043284095e-05, 2.4219269229612533e-10, 2.601563541560286e-06]\n",
            "Ensemble Probabilities: [0.20500206321101785, 0.2573701544365352, 0.007619776384263063, 7.999535946228614e-06]\n",
            "Ensemble Predicted Emotion: sadness\n",
            "[0.52, 0.3999999999999999]\n",
            "/content/drive/MyDrive/FYP_Text/audio_dataset2/neutral\n",
            "[8.40001972914315e-11, 2.781350957505855e-11, 0.9999999905994498, 9.288736473471648e-09]\n",
            "['Neutral']\n",
            "neutral\n",
            "anger\n",
            "Probabilities (Model 1): [3.2461823593621375e-06, 3.053321506740758e-06, 1.1958233017139719e-06, 0.9999924898147583]\n",
            "Probabilities (Model 2): [8.40001972914315e-11, 2.781350957505855e-11, 0.9999999905994498, 9.288736473471648e-09]\n",
            "Ensemble Probabilities: [8.440242134736141e-07, 7.938691544545121e-07, 0.20000030903394836, 0.25999804920958447]\n",
            "Ensemble Predicted Emotion: neutral\n",
            "[0.53, 0.3899999999999999]\n",
            "[1.0149421990055306e-07, 7.778622079014882e-07, 0.9998401617195095, 0.0001589589240627459]\n",
            "['Neutral']\n",
            "sadness\n",
            "anger\n",
            "Probabilities (Model 1): [1.3097953797114315e-06, 0.9999946355819702, 1.3917339174440713e-06, 2.672443770279642e-06]\n",
            "Probabilities (Model 2): [1.0149421990055306e-07, 7.778622079014882e-07, 0.9998401617195095, 0.0001589589240627459]\n",
            "Ensemble Probabilities: [3.668871485041372e-07, 0.2649987301123527, 0.19496920034479243, 3.1705187791359546e-05]\n",
            "Ensemble Predicted Emotion: sadness\n",
            "[0.52, 0.3799999999999999]\n",
            "[0.000869704251957776, 0.0003885171392811598, 0.997799284309107, 0.0009424942996540592]\n",
            "['Neutral']\n",
            "neutral\n",
            "anger\n",
            "Probabilities (Model 1): [1.2148863788752351e-05, 1.960500048880931e-05, 1.247834643436363e-05, 0.9999557733535767]\n",
            "Probabilities (Model 2): [0.000869704251957776, 0.0003885171392811598, 0.997799284309107, 0.0009424942996540592]\n",
            "Ensemble Probabilities: [0.000168402512457053, 7.891555659051077e-05, 0.18958510838880321, 0.2601675749888642]\n",
            "Ensemble Predicted Emotion: neutral\n",
            "[0.53, 0.3699999999999999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcVfp8SbzSmv"
      },
      "outputs": [],
      "source": [
        "# Assuming device is either \"cpu\" or \"cuda:0\"\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA3BUwX7zd0u",
        "outputId": "a6e1fc62-bb5e-4738-d5fb-afb9540b6919"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelBinarizer from version 1.3.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MLPClassifier from version 1.3.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "# Load pretrained models for vocal tone classification and text classification\n",
        "vocal_tone_model = joblib.load(\"TrainedUsing95percCommonData&IncLearnedUsingPersAllDataBatchSize1.pkl\")\n",
        "text_model = joblib.load(\"/content/drive/MyDrive/FYP_Text/emotion_text_classification_model.pkl\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jvQAuSozlzg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Define Fusion Module for ensembling\n",
        "class FusionModule:\n",
        "    def __init__(self, vocal_tone_model, text_model, tokenizer):\n",
        "        self.vocal_tone_model = vocal_tone_model\n",
        "        self.text_model = text_model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def combine_predictions(self, vocal_tone_features, text_features):\n",
        "        # Predictions from vocal tone model\n",
        "        vocal_tone_prediction = self.vocal_tone_model.predict(vocal_tone_features.reshape(1, -1))\n",
        "\n",
        "        # Predictions from text model\n",
        "        text_inputs = self.tokenizer.encode_plus(\n",
        "            text_features,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        text_inputs = {k: v.squeeze(0) for k, v in text_inputs.items()}\n",
        "        text_prediction = self.text_model(**text_inputs)[0]\n",
        "        text_prediction = torch.argmax(text_prediction).item()\n",
        "\n",
        "        # Combine predictions (e.g., simple averaging)\n",
        "        fused_prediction = (vocal_tone_prediction + text_prediction) / 2  # Simple averaging\n",
        "\n",
        "        return fused_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NgXyhOgzqg4"
      },
      "outputs": [],
      "source": [
        "# Predictions from vocal tone model\n",
        "vocal_tone_features = extract_feature('Happy_out_8.wav', mfcc=True, chroma=True, mel=True)\n",
        "vocal_tone_prediction = vocal_tone_model.predict(vocal_tone_features.reshape(1, -1))\n",
        "\n",
        "# # Predictions from text model\n",
        "# text_inputs = self.tokenizer.encode_plus(\n",
        "#     text_features,\n",
        "#     add_special_tokens=True,\n",
        "#     max_length=512,\n",
        "#     padding='max_length',\n",
        "#     return_attention_mask=True,\n",
        "#     return_tensors='pt'\n",
        "# )\n",
        "# text_inputs = {k: v.squeeze(0) for k, v in text_inputs.items()}\n",
        "# text_prediction = self.text_model(**text_inputs)[0]\n",
        "# text_prediction = torch.argmax(text_prediction).item()\n",
        "\n",
        "# # Combine predictions (e.g., simple averaging)\n",
        "# fused_prediction = (vocal_tone_prediction + text_prediction) / 2  # Simple averaging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2RwMB0w3ZLs"
      },
      "outputs": [],
      "source": [
        "text_features = tokenizer('This is great', return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1KZ4-5G3rsg",
        "outputId": "d11d174f-8f64-43b1-a332-45806e4193d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "# Example code to get text features\n",
        "sentence = \"I am feeling happy today!\"\n",
        "text_features = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vocal tone --> Text conversion**"
      ],
      "metadata": {
        "id": "DX1bIXjc86xn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "oSd1hYPASD8Z"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "def audio_to_text(audio_file):\n",
        "    # Initialize the recognizer\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    # Load audio file\n",
        "    with sr.AudioFile(audio_file) as source:\n",
        "        # Adjust for ambient noise\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "\n",
        "        # Listen for speech and transcribe\n",
        "        audio_data = recognizer.record(source)\n",
        "        try:\n",
        "            text = recognizer.recognize_google(audio_data)\n",
        "            return text\n",
        "        except sr.UnknownValueError:\n",
        "            print(\"Speech Recognition could not understand audio\")\n",
        "            return None\n",
        "        except sr.RequestError as e:\n",
        "            print(\"Could not request results from Speech Recognition service; {0}\".format(e))\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQAS2TPESNdN",
        "outputId": "a0611562-702b-4c37-e789-0bd1f06e598b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.3-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
            "Installing collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.3\n"
          ]
        }
      ],
      "source": [
        "%pip install SpeechRecognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-59tAUiWbup",
        "outputId": "707647b5-1e70-49d7-a12f-5a0db813a7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:/Users/HP/Downloads/audio_dataset\\Happy\n",
            "Transcribed Text: it's such a great day spend a time\n",
            "Transcribed Text: office recording let's\n",
            "Transcribed Text: Sunny Deol beaches you happiness\n",
            "Transcribed Text: very very beautiful day I love this yeah of course it's a very very happy\n",
            "Transcribed Text: very very funny right yeah he is a very very funny person I like that I\n",
            "Transcribed Text: Google you are really cool person I like you I want to\n",
            "Transcribed Text: yes of course it's really nice I do like that I want\n",
            "Transcribed Text: understand that your school completing\n",
            "Transcribed Text: give me a sense of achievement feeling the worth of the son of my skin makes\n",
            "Transcribed Text: money in my pocket that I forgot about places of Christ\n",
            "Transcribed Text: it's really good but since you thank you from some\n",
            "Transcribed Text: always make me happy sing my favourite man performs live version\n",
            "Transcribed Text: feel appreciate find the money in my pocket that I forgot about\n",
            "Transcribed Text: I can't help but smile I can't help but\n",
            "Transcribed Text: Kabaddi\n",
            "Transcribed Text: Pritam all friend always brings you are comparing a task I have been\n",
            "Transcribed Text: listen to my favourite podcast audiobook briefly Joker\n",
            "Transcribed Text: I can't stop smiling of receiving such a thoughtful gift the sun\n",
            "Transcribed Text: feeling grateful for another beautiful day I love spend in time with\n",
            "Transcribed Text: you always make me happy getting a hug from friend can bright\n",
            "Transcribed Text: best goofy and its never fail I feel very very happy\n",
            "Transcribed Text: friend can write my older I just got promoted at work it's such a nice day\n",
            "Transcribed Text: between next day very very happy watching a funny movie always lift nice spirits\n",
            "Transcribed Text: go to relaxing weekend I call completing\n",
            "C:/Users/HP/Downloads/audio_dataset\\Sad\n",
            "Transcribed Text: I miss you so much I wish you here with me\n",
            "Transcribed Text: 3GP sad feeling music\n",
            "Transcribed Text: rain bring a loss but can bring\n",
            "Transcribed Text: let someone down is hard to\n",
            "Transcribed Text: Steel failing feels discourage feelings\n",
            "Transcribed Text: in penis being\n",
            "Transcribed Text: feeling like nobody cares can be over\n",
            "Transcribed Text: you feeling like you are not enough for someone is\n",
            "Transcribed Text: feeling like you are stuck in a rock with dest\n",
            "Transcribed Text: can you sing an opportunity you really want that is disappointed\n",
            "Transcribed Text: trusted betray us credit\n",
            "Transcribed Text: hotspot sometimes it Feels Like Nobody understands how\n",
            "Transcribed Text: not knowing where to turn is Sydney\n",
            "Transcribed Text: Salman Khan is very being\n",
            "Transcribed Text: care about me heads\n",
            "Transcribed Text: play someone I translate disappointed feeling like I am waiting\n",
            "Transcribed Text: realising I have grown apart from my child\n",
            "Transcribed Text: it's difficult to keep doing when everything seems to be falling\n",
            "Transcribed Text: since we consume and more each\n",
            "Transcribed Text: missing someone you love birds sometimes it feel\n",
            "Transcribed Text: waking up from good dream and real estate\n",
            "Transcribed Text: where it is saying goodbye to someone you care about\n",
            "Transcribed Text: feeling lonely in a crowded rupees heart break\n",
            "Transcribed Text: photo and missing the good dinosaur\n",
            "C:/Users/HP/Downloads/audio_dataset\\Angry\n",
            "Error occurred while processing: C:/Users/HP/Downloads/audio_dataset\\Angry\\Angry_out_1.wav\n",
            "Transcribed Text: I don't know why can't understand this situation\n",
            "Transcribed Text: I hate you\n",
            "Transcribed Text: no\n",
            "Transcribed Text: make my\n",
            "Transcribed Text: please stop this nonsense there's no point of doing this\n",
            "Transcribed Text: I hate you I really really hate you\n",
            "Transcribed Text: you are really\n",
            "Transcribed Text: what is my opinions by hook or by crook\n",
            "Transcribed Text: you are really funny I am really\n",
            "Transcribed Text: analyse the people who talk loudly in public place\n",
            "Error occurred while processing: C:/Users/HP/Downloads/audio_dataset\\Angry\\Angry_out_2.wav\n",
            "Transcribed Text: I get angry\n",
            "Transcribed Text: why did you do that to me I hate you\n",
            "Transcribed Text: please listen to it on list of India\n",
            "Transcribed Text: you are making me really really angry\n",
            "Transcribed Text: I could string how would you do waste to me\n",
            "Error occurred while processing: C:/Users/HP/Downloads/audio_dataset\\Angry\\Angry_out_3.wav\n",
            "Error occurred while processing: C:/Users/HP/Downloads/audio_dataset\\Angry\\Angry_out_4.wav\n",
            "Transcribed Text: I can't send people to gain my back\n",
            "Transcribed Text: I get angry now\n",
            "Transcribed Text: be with you this terrific\n",
            "Transcribed Text: I am angry at dangerous to support toll I can't\n",
            "Transcribed Text: if someone cancel the plan\n",
            "C:/Users/HP/Downloads/audio_dataset\\Neutral\n",
            "Error occurred while processing: C:/Users/HP/Downloads/audio_dataset\\Neutral\\Neutral_out_1.m4a\n",
            "Transcribed Text: about situation and auto trade and not\n",
            "Transcribed Text: Yes Bank slow\n",
            "Transcribed Text: I don't have strong feelings for me\n",
            "Transcribed Text: schedules pretty average Avengers\n",
            "Transcribed Text: last and responsibilities most strong emotion to report today\n",
            "Transcribed Text: posting through the day nothing\n",
            "Transcribed Text: taking things one step at time\n",
            "Transcribed Text: get out anytime right now just another day not\n",
            "Transcribed Text: just go into\n",
            "Transcribed Text: about the upcoming event\n",
            "Error occurred while processing: C:/Users/HP/Downloads/audio_dataset\\Neutral\\Neutral_out_2.m4a\n",
            "Transcribed Text: workeout karne the AC not challenge\n",
            "Transcribed Text: tdps patient not\n",
            "Transcribed Text: udati ordinary looking\n",
            "Transcribed Text: trading the meeting later\n",
            "Transcribed Text: any particular outcome right now\n",
            "Error occurred while processing: C:/Users/HP/Downloads/audio_dataset\\Neutral\\Neutral_out_3.m4a\n",
            "Error occurred while processing: C:/Users/HP/Downloads/audio_dataset\\Neutral\\Neutral_out_4.m4a\n",
            "Transcribed Text: Yadav about a situation the weather today's near about\n",
            "Transcribed Text: contact DC and see what happens today size in\n",
            "Transcribed Text: I am feeling OK nothing special happen\n",
            "Transcribed Text: parrot non spinal about today's plans Just Another Day In\n",
            "Transcribed Text: nothing predictor interested on my mind I am\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "emotions = {\"happiness\": 0, \"sadness\": 1, \"anger\": 2, \"neutral\": 3}\n",
        "observed_emotions = ['Happy', 'Sad', 'Angry', 'Neutral']\n",
        "\n",
        "x, y_label = [], []\n",
        "folder_path = \"C:/Users/HP/Downloads/audio_dataset\"\n",
        "for folder in observed_emotions:\n",
        "    filepath = os.path.join(folder_path, folder)\n",
        "    print(filepath)\n",
        "    for filename in os.listdir(filepath):\n",
        "        # print(os.path.join(filepath, filename))\n",
        "        hj = os.path.join(filepath, filename)\n",
        "        try:\n",
        "\n",
        "            transcribed_text = audio_to_text(hj)\n",
        "            if transcribed_text:\n",
        "                print(\"Transcribed Text:\", transcribed_text)\n",
        "                x.append(transcribed_text)\n",
        "                y_label.append(folder)\n",
        "        except:\n",
        "            print(\"Error occurred while processing:\", hj)\n",
        "        # feature = extract_feature(os.path.join(filepath, filename), mfcc=True, chroma=True, mel=True)\n",
        "        # x.append(feature)\n",
        "        # y_true.append(folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7g_RpIJ6dap"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(list(zip(x, y_label)), columns=['content', 'emotion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ppc26vr6daq",
        "outputId": "2b6104a2-b706-4626-bba2-f6debe1d498a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>it's such a great day spend a time</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>office recording let's</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sunny Deol beaches you happiness</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>very very beautiful day I love this yeah of co...</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>very very funny right yeah he is a very very f...</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>Yadav about a situation the weather today's ne...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>contact DC and see what happens today size in</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>I am feeling OK nothing special happen</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>parrot non spinal about today's plans Just Ano...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>nothing predictor interested on my mind I am</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>88 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              content  emotion\n",
              "0                  it's such a great day spend a time    Happy\n",
              "1                              office recording let's    Happy\n",
              "2                    Sunny Deol beaches you happiness    Happy\n",
              "3   very very beautiful day I love this yeah of co...    Happy\n",
              "4   very very funny right yeah he is a very very f...    Happy\n",
              "..                                                ...      ...\n",
              "83  Yadav about a situation the weather today's ne...  Neutral\n",
              "84      contact DC and see what happens today size in  Neutral\n",
              "85             I am feeling OK nothing special happen  Neutral\n",
              "86  parrot non spinal about today's plans Just Ano...  Neutral\n",
              "87       nothing predictor interested on my mind I am  Neutral\n",
              "\n",
              "[88 rows x 2 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScxoeIoI6dar"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"transcribed.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDacHULp6das"
      },
      "outputs": [],
      "source": [
        "# # Assuming have functions to load text and audio data, and to perform speech-to-text conversion and feature extraction\n",
        "\n",
        "# # Step 1: Load data\n",
        "# text_data = load_text_data(\"text_samples.csv\")\n",
        "# audio_data = load_audio_data(\"audio_samples.wav\")\n",
        "\n",
        "# # Step 2: Convert audio data into text\n",
        "# converted_text_data = []\n",
        "# for audio_sample in audio_data:\n",
        "#     converted_text = audio_to_text(audio_sample)\n",
        "#     converted_text_data.append(converted_text)\n",
        "\n",
        "# # Step 3: Extract features\n",
        "# text_features = extract_text_features(text_data)\n",
        "# audio_features = extract_audio_features(converted_text_data)\n",
        "\n",
        "# # Step 4: Make initial predictions\n",
        "# probabilities1, probabilities2 = model_predictions(text_features, audio_features)\n",
        "\n",
        "# # Step 5: Gather user feedback or ground truth labels\n",
        "# user_labels = get_user_labels(text_data, audio_data)  # Assuming you have a way to get user feedback\n",
        "\n",
        "# # Step 6: Update model weights based on feedback\n",
        "# updated_probabilities1, updated_probabilities2, ensemble_probabilities, predicted_emotion, model_weights = ensemble_predictions(probabilities1, probabilities2, user_labels, model_weights, learning_rate=0.1)\n",
        "\n",
        "# # Step 7: Repeat for each sample\n",
        "# # This step depends on the size of your dataset and the available computational resources\n",
        "\n",
        "# # Step 8: Evaluate performance on a validation set\n",
        "# validation_text_data = load_text_data(\"validation_text.csv\")\n",
        "# validation_audio_data = load_audio_data(\"validation_audio.wav\")\n",
        "# validation_user_labels = get_user_labels(validation_text_data, validation_audio_data)\n",
        "# validation_text_features = extract_text_features(validation_text_data)\n",
        "# validation_audio_features = extract_audio_features(validation_audio_data)\n",
        "# validation_probabilities1, validation_probabilities2 = model_predictions(validation_text_features, validation_audio_features)\n",
        "# accuracy = evaluate(validation_probabilities1, validation_probabilities2, validation_user_labels)\n",
        "# print(\"Accuracy:\", accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}